import time, json, requests
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

# 1. ç‰©ç†éš”ç¦»ä¸ç¯å¢ƒ
BASE_DIR = Path(__file__).parent
CONFIG = json.loads((BASE_DIR / "config.json").read_text(encoding="utf-8"))
REPORTS_DIR = BASE_DIR / "reports"
PROTOCOL = (BASE_DIR / "auditor_protocol.md").read_text(encoding="utf-8")

# 2. æˆ˜æœ¯å·¥å…·ï¼šå®šå‘æœç´¢ä¸å¼ºåˆ¶æŠ“å–
def fetch_intelligence(query, num=10):
    # é€»è¾‘ï¼šåªæœç¡¬æ ¸ç¤¾åŒº (V2EX, Reddit, GitHub, HuggingFace)
    site_filter = " (site:v2ex.com OR site:reddit.com OR site:github.com OR site:huggingface.co)"
    search_payload = {"q": query + site_filter, "num": num}
    search_res = requests.post(CONFIG["SERPER_ENDPOINT"],
                               headers={"X-API-KEY": CONFIG["SERPER_API_KEY"]},
                               json=search_payload).json()
    
    black = ["awesome", "list", "collection", "resource", "directory"]
    organic = search_res.get("organic", []) or []
    scored = []
    for it in organic:
        link = it.get("link") or ""
        title = (it.get("title") or "").lower()
        url_low = link.lower()
        if any(b in title or b in url_low for b in black):
            continue
        score = 0
        if "huggingface.co/spaces" in link:
            score += 10
        if "replicate.com" in link:
            score += 8
        if "github.com" in link:
            if "readme" in url_low or "#readme" in url_low:
                score += 7
            else:
                score += 4
        scored.append((score, link))
    scored.sort(reverse=True)
    links = [l for _, l in scored]
    results = []
    
    with ThreadPoolExecutor(max_workers=2) as ex:
        futures = {ex.submit(scrape_and_audit, link): link for link in links}
        for fut in as_completed(futures):
            res = fut.result()
            if res:
                results.append(res)
            time.sleep(1.5)
    return results


def scrape_and_audit(url):
    # åŠ¨ä½œï¼šæŠ“å–å¹¶å¼ºåˆ¶è¦æ±‚æå–å¤šåª’ä½“é“¾æ¥
    scrape_payload = {"url": url, "formats": ["markdown", "links"]}
    r = requests.post(
        CONFIG["FIRECRAWL_ENDPOINT"],
        headers={"Authorization": f"Bearer {CONFIG['FIRECRAWL_API_KEY']}"},
        json=scrape_payload,
    )
    time.sleep(3)
    if r.status_code != 200:
        return None
    
    data_json = r.json().get("data", {}) or {}
    md_content = data_json.get("markdown", "") or ""
    text_low = md_content.lower()
    # èº«ä»½æ ¸éªŒï¼šéè¯­éŸ³/éŸ³é¢‘ç›´æ¥åˆ¤å®šä¸º INVALID_LEAD
    status = deepseek_validate(md_content, CONFIG)
    if status != "VALID_AUDIO":
        return {"status": "INVALID_LEAD", "url": url}
    raw_links = data_json.get("links") or []
    candidates = []
    for lk in raw_links:
        href = lk if isinstance(lk, str) else (lk.get("url") or "")
        text = "" if isinstance(lk, str) else (lk.get("text") or "")
        score = 0
        if any(k in (text or "").lower() for k in ["demo", "try", "live", "playground", "examples"]):
            score += 5
        if "huggingface.co/spaces" in href:
            score += 4
        if "github.com" in href and href.count("/") >= 4:  # repo
            score += 3
        if "youtube.com" in href or "youtu.be" in href:
            score += 2
        if score > 0:
            candidates.append((score, href))
    candidates.sort(reverse=True)
    preview = candidates[0][1] if candidates else ""
    if not preview:
        # è‹¥ä¸ºç›®å½•ç±»ä½†å­˜åœ¨å¯ç”¨ Demoï¼Œåˆ™ä¿ç•™ï¼›å¦åˆ™åºŸå¼ƒ
        if any(x in text_low for x in ["awesome", "èµ„æºåˆ—è¡¨", "åˆé›†", "ç›®å½•", "åˆ—è¡¨"]):
            return None
        if ("huggingface.co/spaces" in url) or ("github.com" in url):
            preview = url
    # è°ƒç”¨ DeepSeek å®¡è®¡ï¼šåªè¦çœŸè¿¹ï¼Œä¸è¦æ€»ç»“
    audit_payload = {
        "model": CONFIG["DEEPSEEK_MODEL"],
        "messages": [
            {"role": "system", "content": PROTOCOL},
            {"role": "user", "content": f"ç´ ææ¥è‡ª: {url}\n\nå†…å®¹: {md_content[:8000]}"},
        ],
    }
    audit_res = requests.post(
        f"{CONFIG['DEEPSEEK_ENDPOINT']}/chat/completions",
        headers={"Authorization": f"Bearer {CONFIG['DEEPSEEK_API_KEY']}"},
        json=audit_payload,
    ).json()
    audit_text = audit_res["choices"][0]["message"]["content"]
    problem = deepseek_explain(md_content, CONFIG)
    metrics = deepseek_metrics(md_content, CONFIG)
    return {"status": "VALID_AUDIO", "url": url, "preview": preview, "audit": audit_text, "problem": problem, "metrics": metrics}


def main():
    print("ğŸš€ å¯åŠ¨è¯­éŸ³å‚ç›´å®æ•ˆæ”¶å‰²...")
    REPORTS_DIR.mkdir(exist_ok=True)
    tasks = [
        {"query": "Fish-Speech å•†ä¸šåŒ–å®æµ‹ è¯­éŸ³å…‹éš† æ•ˆæœæ¼”ç¤º site:v2ex.com", "category": "Fish-Speech"},
        {"query": "GPT-SoVITS è¿˜åŸåº¦ é¿å‘æŒ‡å— å˜ç°æ–¹æ¡ˆ site:reddit.com", "category": "GPT-SoVITS"},
        {"query": "ElevenLabs æ›¿ä»£å“ å¼€æº å®æ—¶ç¿»è¯‘ æ•ˆæœå¯¹æ¯” site:huggingface.co", "category": "ElevenLabs æ›¿ä»£"},
    ]
    fallback_seeds = {
        "Fish-Speech": ["https://huggingface.co/spaces/fishaudio/fish-speech"],
        "GPT-SoVITS": ["https://github.com/RVC-Boss/GPT-SoVITS"],
        "ElevenLabs æ›¿ä»£": ["https://huggingface.co/spaces/Plachta/Realtime-TTS"]
    }
    
    items = []
    for t in tasks:
        print(f"ğŸ“¡ æ­£åœ¨æ”¶å‰²ç±»ç›®: {t['category']}")
        res = fetch_intelligence(t["query"], num=10)
        invalid = sum(1 for it in res if isinstance(it, dict) and it.get("status") == "INVALID_LEAD")
        valid = [it for it in res if isinstance(it, dict) and it.get("status") != "INVALID_LEAD"]
        if invalid >= 8 and len(valid) < 3:
            # è‡ªåŠ¨æ‰©å±•æœç´¢æ·±åº¦
            res = fetch_intelligence(t["query"], num=30)
            valid = [it for it in res if isinstance(it, dict) and it.get("status") != "INVALID_LEAD"]
        for it in res:
            if isinstance(it, dict):
                it["category"] = t["category"]
                if it.get("status") != "INVALID_LEAD":
                    items.append(it)
        if not any(x.get("category")==t["category"] for x in items):
            # ä½¿ç”¨ç§å­é“¾æ¥ç¡®ä¿è‡³å°‘ä¸€æ¡æ¼”ç¤º
            for u in fallback_seeds.get(t["category"], []):
                tmp = scrape_and_audit(u)
                if isinstance(tmp, dict):
                    tmp["category"] = t["category"]
                    if tmp.get("status") != "INVALID_LEAD":
                        items.append(tmp)
                    break
    
    date_str = time.strftime("%Y-%m-%d")
    title = f"AI è¯­éŸ³å®æ•ˆæƒ…æŠ¥ {date_str} [Fish-Speech | GPT-SoVITS | ElevenLabs æ›¿ä»£]"
    def trunc(s, n=250):
        return s[:n]
    parts = []
    parts.append(f"---\nmarp: true\ntitle: {title}\n---\n# {title}\n\næ”¶å‰²æ¡ç›®ï¼š{len(items)}")
    for it in items:
        body1 = f"- è§£å†³äº†ä»€ä¹ˆ: {trunc(it.get('problem',''), 200)}\n- æ•ˆæœæŒ‡æ ‡: {trunc(it.get('metrics',''), 40)}\n- æˆå“é¢„è§ˆ: {it['preview']}\n- æ¥æº: {it['url']}"
        parts.append(f"---\n# {it['category']}\n\n{trunc(body1, 250)}")
        audit_txt = it.get("audit","")
        if audit_txt:
            # åˆ†é¡µå±•ç¤ºå®¡è®¡å†…å®¹ï¼Œé¿å…åˆ å‡
            chunk = 240
            for i in range(0, len(audit_txt), chunk):
                parts.append(f"---\n## å®¡è®¡ç»†èŠ‚\n\n{audit_txt[i:i+chunk]}")
    report_path = REPORTS_DIR / f"voice_intel_{int(time.time())}.md"
    report_path.write_text("\n".join(parts), encoding="utf-8")
    print(f"ğŸ ä»»åŠ¡å®Œæˆã€‚æŠ¥å‘Šè§: {report_path}")
    if items:
        first = items[0]
        print(f"âœ… é¦–æ¡éªŒè¯ | è§£å†³äº†ä»€ä¹ˆ: {first.get('problem','')}")
        print(f"âœ… é¦–æ¡éªŒè¯ | æˆå“é“¾æ¥: {first.get('preview','')}")
        print(f"âœ… é¦–æ¡éªŒè¯ | æ•ˆæœæŒ‡æ ‡: {first.get('metrics','')}")

def deepseek_explain(text: str, config: dict) -> str:
    payload = {
        "model": config["DEEPSEEK_MODEL"],
        "messages": [
            {"role": "system", "content": f"{PROTOCOL}\nä»…è¾“å‡ºä¸€å¥è¯ï¼Œè¯´æ˜è¯¥é¡¹ç›®è§£å†³äº†ä»€ä¹ˆè¯­éŸ³éš¾é¢˜ï¼ˆå¦‚æé€Ÿå…‹éš†ã€æƒ…æ„Ÿè¡¨è¾¾ã€å®æ—¶ç¿»è¯‘ï¼‰ã€‚ç¦æ­¢ä½¿ç”¨å½¢å®¹è¯ï¼Œå¿…é¡»å…·ä½“ã€å¯éªŒè¯ã€‚"},
            {"role": "user", "content": text[:6000]}
        ]
    }
    try:
        r = requests.post(f"{config['DEEPSEEK_ENDPOINT']}/chat/completions",
                          headers={"Authorization": f"Bearer {config['DEEPSEEK_API_KEY']}"},
                          json=payload, timeout=60)
        if r.status_code != 200:
            return ""
        js = r.json()
        return js.get("choices",[{}])[0].get("message",{}).get("content","")
    except:
        return ""

def deepseek_validate(text: str, config: dict) -> str:
    payload = {
        "model": config["DEEPSEEK_MODEL"],
        "messages": [
            {"role": "system", "content": "åˆ¤æ–­è¯¥ç´ ææ˜¯å¦å±äºè¯­éŸ³/éŸ³é¢‘å·¥å…·æˆ–æ–¹æ¡ˆã€‚åªè¿”å›ä¸¤ä¸ªè¯ä¹‹ä¸€ï¼šVALID_AUDIO æˆ– INVALID_LEADã€‚"},
            {"role": "user", "content": text[:4000]}
        ]
    }
    try:
        r = requests.post(f"{config['DEEPSEEK_ENDPOINT']}/chat/completions",
                          headers={"Authorization": f"Bearer {config['DEEPSEEK_API_KEY']}"},
                          json=payload, timeout=40)
        if r.status_code != 200:
            return "INVALID_LEAD"
        js = r.json()
        out = js.get("choices",[{}])[0].get("message",{}).get("content","").strip().upper()
        return "VALID_AUDIO" if "VALID_AUDIO" in out else "INVALID_LEAD"
    except:
        return "INVALID_LEAD"

def deepseek_metrics(text: str, config: dict) -> str:
    payload = {
        "model": config["DEEPSEEK_MODEL"],
        "messages": [
            {"role": "system", "content": "ä»…æå–ä¸€å¥å…·ä½“æ•ˆæœæŒ‡æ ‡ï¼ˆå¦‚å»¶è¿Ÿ<100msã€3ç§’ç´ æå¯å¤åˆ»ã€å®æ—¶æ€§90fpsï¼‰ã€‚ç¦æ­¢å½¢å®¹è¯ã€‚å­—æ•°â‰¤40ã€‚"},
            {"role": "user", "content": text[:4000]}
        ]
    }
    try:
        r = requests.post(f"{config['DEEPSEEK_ENDPOINT']}/chat/completions",
                          headers={"Authorization": f"Bearer {config['DEEPSEEK_API_KEY']}"},
                          json=payload, timeout=40)
        if r.status_code != 200:
            return ""
        js = r.json()
        return js.get("choices",[{}])[0].get("message",{}).get("content","").strip()
    except:
        return ""


if __name__ == "__main__":
    main()
