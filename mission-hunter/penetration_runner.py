from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import requests
from typing import List, Dict, Any, Tuple
from blueprint import load_config

BASE_DIR = Path(__file__).parent
CONFIG_PATH = BASE_DIR / "config.json"
REPORTS_DIR = BASE_DIR / "reports"
AUDITOR_PROTOCOL_PATH = BASE_DIR / "auditor_protocol.md"
MAX_THREADS = 2

def ensure_dirs():
    REPORTS_DIR.mkdir(parents=True, exist_ok=True)

def serper_search(query: str, config: Dict[str, Any]) -> List[str]:
    endpoint = config.get("SERPER_ENDPOINT", "")
    key = config.get("SERPER_API_KEY", "")
    payload = {"q": query, "num": 10}
    headers = {"X-API-KEY": key, "Content-Type": "application/json"}
    try:
        r = requests.post(endpoint, headers=headers, json=payload, timeout=20)
        if r.status_code != 200:
            return []
        data = r.json()
        links = []
        for section in ["organic", "news", "videos"]:
            items = data.get(section, [])
            for it in items:
                link = it.get("link") or it.get("url")
                if link:
                    links.append(link)
        return links
    except:
        return []

def firecrawl_markdown(url: str, config: Dict[str, Any]) -> Tuple[str, str]:
    endpoint = config.get("FIRECRAWL_ENDPOINT", "")
    key = config.get("FIRECRAWL_API_KEY", "")
    headers = {"Authorization": f"Bearer {key}", "Content-Type": "application/json"}
    payload = {"url": url, "formats": ["markdown"]}
    try:
        time.sleep(2)
        r = requests.post(endpoint, headers=headers, json=payload, timeout=30)
        if r.status_code != 200:
            return url, ""
        json_res = r.json()
        data_content = json_res.get("data", {})
        md = data_content.get("markdown") or data_content.get("content") or ""
        return url, md
    except:
        return url, ""

def gather_links(queries: List[str], config: Dict[str, Any], limit: int = 50) -> List[str]:
    links = []
    seen = set()
    with ThreadPoolExecutor(max_workers=MAX_THREADS) as ex:
        futures = [ex.submit(serper_search, q, config) for q in queries]
        for fut in as_completed(futures):
            res = fut.result()
            for link in res:
                if link not in seen:
                    seen.add(link)
                    links.append(link)
                if len(links) >= limit:
                    break
            if len(links) >= limit:
                break
    return links

def fetch_markdowns(urls: List[str], config: Dict[str, Any]) -> Dict[str, str]:
    out: Dict[str, str] = {}
    with ThreadPoolExecutor(max_workers=MAX_THREADS) as ex:
        futures = [ex.submit(firecrawl_markdown, u, config) for u in urls]
        for fut in as_completed(futures):
            url, md = fut.result()
            if md:
                out[url] = md
    return out

def truncate_join(md_map: Dict[str, str], per_doc: int = 4000, max_docs: int = 10) -> str:
    agg = []
    count = 0
    for u, m in md_map.items():
        if count >= max_docs:
            break
        agg.append(f"URL: {u}\n\n{m[:per_doc]}")
        count += 1
    return "\n\n---\n\n".join(agg)

def deepseek_table(text: str, protocol: str, config: Dict[str, Any]) -> str:
    endpoint = config.get("DEEPSEEK_ENDPOINT", "")
    route = config.get("DEEPSEEK_ROUTE", "")
    model = config.get("DEEPSEEK_MODEL", "")
    key = config.get("DEEPSEEK_API_KEY", "")
    url = endpoint.rstrip("/") + "/" + route.lstrip("/")
    headers = {"Authorization": f"Bearer {key}", "Content-Type": "application/json"}
    system_prompt = protocol + "\n\nè¯·ä»…è¾“å‡ºä¸€ä¸ª Markdown è¡¨æ ¼ï¼Œåˆ—åŒ…å«ï¼šæ–¹æ¡ˆåã€å®æµ‹æˆåŠŸç‡ã€ä¸‹å¡æ—¶é—´ã€æ˜¯å¦éœ€è¦å®ä½“èµ„äº§ã€å•æ¬¡ä»»åŠ¡æˆæœ¬ã€‚å°½é‡åŸºäºç´ æçš„å¯éªŒè¯æ¥æºï¼Œä¿¡æ¯ä¸è¶³æ—¶åšä¿å®ˆä¼°è®¡å¹¶æ ‡æ³¨ã€‚"
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": text},
    ]
    payload = {"model": model, "messages": messages}
    try:
        r = requests.post(url, headers=headers, json=payload, timeout=60)
        if r.status_code != 200:
            return ""
        data = r.json()
        c = data.get("choices", [])
        if c and c[0].get("message", {}).get("content"):
            return c[0]["message"]["content"]
        return ""
    except:
        return ""

def make_queries() -> List[str]:
    phrases = [
        "Claude æ”¯ä»˜ 3DS ç»•è¿‡",
        "æ— éœ€ä¿¡ç”¨å¡ è®¢é˜… OpenAI 2026",
        "OCBC æ¿€æ´»æ—¶æ•ˆ",
    ]
    sites = ["site:v2ex.com", "site:reddit.com"]
    return [f"{p} {s}" for p in phrases for s in sites]

def main():
    print("ğŸš€ æ·±åº¦æ¸—é€ä»»åŠ¡å¯åŠ¨")
    ensure_dirs()
    config = load_config(CONFIG_PATH)
    print("âœ… é…ç½®åŠ è½½å®Œæˆ")
    queries = make_queries()
    print(f"ğŸ” ç›®æ ‡å…³é”®è¯: {queries}")
    links = gather_links(queries, config, limit=50)
    print(f"ğŸ”— é“¾æ¥æ€»æ•°: {len(links)}")
    if not links:
        print("âŒ æœªå‘ç°ä»»ä½•é“¾æ¥")
        return
    filtered = [u for u in links if ("v2ex.com" in u) or ("reddit.com" in u)]
    if not filtered:
        print("âŒ æœªç­›é€‰åˆ°ç›®æ ‡ç«™ç‚¹é“¾æ¥ï¼ˆv2ex/redditï¼‰")
        return
    print(f"ğŸ•¸ï¸ æ­£åœ¨æŠ“å–å†…å®¹ (çº¿ç¨‹: {MAX_THREADS})")
    md_map = fetch_markdowns(filtered, config)
    print(f"ğŸ“„ æŠ“å–æˆåŠŸæ–‡æ¡£æ•°: {len(md_map)}")
    if not md_map:
        print("âŒ æœªè·å–åˆ°ä»»ä½•ç½‘é¡µå†…å®¹")
        return
    protocol = (AUDITOR_PROTOCOL_PATH.read_text(encoding="utf-8") if AUDITOR_PROTOCOL_PATH.exists() else "")
    material = truncate_join(md_map, per_doc=4000, max_docs=10)
    print("ğŸ§  æ­£åœ¨ç”Ÿæˆå¯è¡Œæ€§å¯¹æ¯”è¡¨")
    table = deepseek_table(material, protocol, config)
    if not table:
        print("âŒ DeepSeek è¿”å›ä¸ºç©º")
        return
    out_path = REPORTS_DIR / "feasibility_table.md"
    out_path.write_text(table, encoding="utf-8")
    print(f"ğŸ å·²è¾“å‡º: {out_path}")

if __name__ == "__main__":
    main()
