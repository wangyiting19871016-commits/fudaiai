#!/usr/bin/env python3
"""
å°çº¢ä¹¦çœŸè¿¹å½’çº³å¼•æ“
åŸºäºPlaywrightå®ç°ï¼ŒåŒ…å«æœç´¢ã€è·å–å…³è”è¯ã€æŠ“å–é«˜èµå¸–å­ç­‰æ ¸å¿ƒåŠŸèƒ½
"""

import asyncio
import os
from collections import Counter
from playwright.async_api import async_playwright, Page
from typing import List, Dict, Any


class XiaoHongShuScraper:
    """å°çº¢ä¹¦çˆ¬è™«æ ¸å¿ƒç±»"""
    
    def __init__(self):
        self.browser = None
        self.context = None
        self.playwright = None
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.playwright = await async_playwright().start()
        
        # 1. ç»å¯¹è·¯å¾„å›ºåŒ–ï¼šå°†user_data_dirè®¾ç½®ä¸ºç»å¯¹è·¯å¾„
        self.user_data_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "xhs_user_data"))
        os.makedirs(self.user_data_dir, exist_ok=True)
        print(f"ğŸ“ ç”¨æˆ·æ•°æ®ç›®å½•: {self.user_data_dir}")
        
        self.context = await self.playwright.chromium.launch_persistent_context(
            user_data_dir=self.user_data_dir,
            headless=False,  # å¼ºåˆ¶æµè§ˆå™¨ä¿æŒå¯è§ï¼Œä¸ç§’é—ª
            args=[
                '--no-sandbox',
                '--disable-dev-shm-usage',
                '--disable-blink-features=AutomationControlled'
            ]
        )
        
        self.browser = self.context.browser
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.context:
            await self.context.close()
        if self.playwright:
            await self.playwright.stop()
    
    async def check_login(self, page: Page) -> bool:
        """æ£€æŸ¥ç™»å½•çŠ¶æ€ï¼Œæ£€æµ‹ç™»å½•å¼¹çª—å¹¶ç­‰å¾…äººå·¥å¤„ç†"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç™»å½•å¼¹çª—
            login_selectors = [
                "//div[contains(@class, 'login-modal')]",
                "//div[contains(text(), 'ç™»å½•')]/ancestor::div[contains(@class, 'modal')]",
                "//div[@class*='login-container']",
                "//button[contains(text(), 'ç«‹å³ç™»å½•')]",
                "//img[contains(@src, 'qr-code')]"
            ]
            
            for selector in login_selectors:
                try:
                    await page.locator(selector).first.wait_for(timeout=3000)
                    print("âš ï¸  æ£€æµ‹åˆ°ç™»å½•å¼¹çª—")
                    input("è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•ï¼ŒæŒ‰å›è½¦é”®ç»§ç»­ç¨‹åº...")
                    return False  # è¿”å›Falseè¡¨ç¤ºéœ€è¦é‡æ–°æ£€æŸ¥
                except:
                    continue
            
            return True  # è¿”å›Trueè¡¨ç¤ºå·²ç™»å½•
            
        except Exception as e:
            print(f"æ£€æŸ¥ç™»å½•çŠ¶æ€å¤±è´¥: {e}")
            return False
    
    async def search(self, keyword: str) -> Page:
        """æ‰§è¡Œæœç´¢æ“ä½œ"""
        page = await self.context.new_page()
        
        # ç›´æ¥è·³è½¬åˆ°æœç´¢ç»“æœé¡µ
        search_url = f"https://www.xiaohongshu.com/search_result?keyword={keyword}"
        await page.goto(search_url)
        await page.wait_for_load_state("domcontentloaded")
        await page.wait_for_timeout(5000)  # é™é€Ÿä¿ç¨³
        
        # æ£€æŸ¥ç™»å½•çŠ¶æ€
        print("ğŸ” æ£€æŸ¥ç™»å½•çŠ¶æ€...")
        is_logged_in = await self.check_login(page)
        if not is_logged_in:
            # å†æ¬¡æ£€æŸ¥ç™»å½•çŠ¶æ€
            await page.wait_for_load_state("domcontentloaded")
            await page.wait_for_timeout(5000)  # é™é€Ÿä¿ç¨³
            await self.check_login(page)
        
        return page
    
    async def get_related_search_terms(self, page: Page) -> List[str]:
        """è·å–å…³è”æœç´¢è¯ - ä»50ä¸ªçˆ†æ¬¾å¸–æ ‡é¢˜ä¸­æçº¯å‡º10å¤§çƒ­é—¨é£æ ¼"""
        try:
            print("ğŸ” å¼€å§‹è·å–å…³è”æœç´¢è¯...")
            
            # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆï¼Œé™é€Ÿä¿ç¨³
            await page.wait_for_load_state("domcontentloaded")
            await page.wait_for_timeout(5000)
            
            # 1. åˆæ¬¡æŒ–æ˜ï¼šæŠ“å–å‰50æ¡å¸–å­çš„title
            print("ğŸ“‹ å¼€å§‹æŠ“å–å‰50æ¡å¸–å­çš„æ ‡é¢˜...")
            titles = []
            scroll_count = 0
            max_scrolls = 20
            
            while len(titles) < 50 and scroll_count < max_scrolls:
                print(f"ğŸ”„ æ»šåŠ¨ç¬¬{scroll_count+1}/{max_scrolls}æ¬¡ï¼Œå·²æŠ“å–{len(titles)}/50ä¸ªæ ‡é¢˜")
                
                # æ»šåŠ¨1500åƒç´ 
                await page.evaluate("window.scrollBy(0, 1500)")
                await page.wait_for_load_state("domcontentloaded")
                await page.wait_for_timeout(5000)  # é™é€Ÿä¿ç¨³
                
                # å®šä½å¸–å­å®¹å™¨
                post_cards = await page.locator(".note-item, section").all()
                
                for card in post_cards:
                    if len(titles) >= 50:
                        break
                    
                    try:
                        # æå–æ ‡é¢˜
                        title = ""
                        try:
                            title = await card.locator("span.title").inner_text(timeout=5000)
                        except:
                            try:
                                title = await card.locator(".footer .title").inner_text(timeout=5000)
                            except:
                                title = "æ— æ ‡é¢˜"
                        
                        if title and title != "æ— æ ‡é¢˜" and title not in titles:
                            titles.append(title)
                    except Exception as e:
                        print(f"âš ï¸  æå–æ ‡é¢˜å¤±è´¥: {e}")
                        continue
                
                scroll_count += 1
            
            print(f"âœ… æˆåŠŸæŠ“å–{len(titles)}/50ä¸ªå¸–å­æ ‡é¢˜")
            
            # 2. è¯é¢‘ç»Ÿè®¡ï¼šå¯¹50ä¸ªæ ‡é¢˜è¿›è¡Œç®€å•çš„è¯é¢‘åˆ†æ
            print("ğŸ“Š å¼€å§‹è¿›è¡Œè¯é¢‘åˆ†æ...")
            
            # å¸¸è§åœç”¨è¯åˆ—è¡¨
            stopwords = {
                'è°ƒè‰²', 'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸',
                'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦',
                'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™',
                'æˆ‘ä»¬', 'æ¥', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰',
                'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'æˆ‘ä»¬', 'æ¥', 'åˆ', 'å¤š', 'å', 'å½“',
                'é‡Œ', 'ç”¨', 'ç°åœ¨', 'å¥½', 'åª', 'å¯', 'åˆ', 'æ²¡', 'ä¸º', 'å’Œ',
                'é‚£', 'å—', 'å‘¢', 'è®©', 'æˆ‘', 'è¿™', 'ä¸', 'ä½ ', 'éƒ½', 'ä¸Š',
                'çœ‹', 'æ¥', 'å¥½', 'è‡ªå·±', 'è¿™', 'æˆ‘ä»¬', 'æ¥', 'åˆ°', 'è¯´', 'è¦',
                'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™',
                'ç»¼åˆ', 'å…¨éƒ¨', 'æœ€æ–°', 'æœ€çƒ­', 'å¤§å®¶', 'å–œæ¬¢', 'åˆ†äº«',
                'æ•™ç¨‹', 'æ–¹æ³•', 'æŠ€å·§', 'ç®€å•', 'å¿«é€Ÿ', 'å®ç”¨', 'å¥½çœ‹',
                'æ•ˆæœ', 'è¶…', 'å¤ª', 'å¾ˆ', 'éå¸¸', 'ç‰¹åˆ«', 'è¶…çº§', 'ç»å¯¹',
                'ç®€ç›´', 'çœŸçš„', 'ä¸€å®š', 'å¯ä»¥', 'èƒ½å¤Ÿ', 'å°±æ˜¯', 'è¿˜æ˜¯',
                'ä½†æ˜¯', 'ä¸è¿‡', 'æ‰€ä»¥', 'å› ä¸º', 'å¦‚æœ', 'è™½ç„¶', 'ä½†æ˜¯',
                'è€Œä¸”', 'å¹¶ä¸”', 'æˆ–è€…', 'ä¸æ˜¯', 'è€Œæ˜¯', 'æ‰€ä»¥', 'å› ä¸º',
                'å¦‚æœ', 'è™½ç„¶', 'ä½†æ˜¯', 'è€Œä¸”', 'å¹¶ä¸”', 'æˆ–è€…', 'ä¸æ˜¯',
                'è€Œæ˜¯', 'è¿™é‡Œ', 'é‚£é‡Œ', 'è¿™æ ·', 'é‚£æ ·', 'æ€ä¹ˆ', 'ä»€ä¹ˆ',
                'å“ªé‡Œ', 'ä¸ºä»€ä¹ˆ', 'å¤šå°‘', 'æ—¶å€™', 'ä»Šå¤©', 'æ˜å¤©', 'æ˜¨å¤©',
                'æ™šä¸Š', 'æ—©ä¸Š', 'ä¸­åˆ', 'ä¸‹åˆ', 'ä»Šå¹´', 'å»å¹´', 'æ˜å¹´',
                'ä»¥å', 'ä»¥å‰', 'ç°åœ¨', 'å°†æ¥', 'è¿‡å»', 'å¼€å§‹', 'ç»“æŸ',
                'ç„¶å', 'æœ€å', 'é¦–å…ˆ', 'å…¶æ¬¡', 'ç¬¬ä¸‰', 'ç¬¬å››', 'ç¬¬äº”',
                'ç¬¬å…­', 'ç¬¬ä¸ƒ', 'ç¬¬å…«', 'ç¬¬ä¹', 'ç¬¬å'
            }
            
            # æå–é£æ ¼è¯
            style_words = []
            for title in titles:
                # ç®€å•çš„åˆ†è¯ï¼šæŒ‰ç©ºæ ¼ã€æ ‡ç‚¹ç¬¦å·ç­‰åˆ†å‰²
                # è¿™é‡Œä½¿ç”¨ç®€å•çš„åˆ†å‰²æ–¹å¼ï¼Œå®é™…å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„åˆ†è¯åº“
                words = title.split()
                for word in words:
                    # æ¸…æ´—è¯è¯­ï¼šå»é™¤æ ‡ç‚¹ç¬¦å·ï¼Œè½¬æ¢ä¸ºå°å†™
                    cleaned_word = ''.join(c for c in word if c.isalnum())
                    if cleaned_word and len(cleaned_word) > 1 and cleaned_word not in stopwords:
                        style_words.append(cleaned_word)
            
            # 3. æŒ‰é¢‘ç‡æ’åºï¼šé€‰å‡ºå‡ºç°é¢‘ç‡æœ€é«˜çš„å‰10ä¸ªè¯
            word_counts = Counter(style_words)
            top_words = [word for word, count in word_counts.most_common(10)]
            
            # å¦‚æœè¯æ•°ä¸è¶³10ä¸ªï¼Œä½¿ç”¨å…œåº•å…³é”®è¯
            if len(top_words) < 10:
                fallback_terms = [
                    'å¯Œå£«NCè°ƒè‰²', 'è«å…°è¿ªè‰²ç³»', 'æ³•å¼å¤å¤æ»¤é•œ', 'èƒ¶ç‰‡æ„Ÿè°ƒè‰²',
                    'insé£æ»¤é•œ', 'å†·ç™½çš®è°ƒè‰²', 'å¥¶æ²¹è‚Œæ»¤é•œ', 'èµ›åšæœ‹å…‹è°ƒè‰²',
                    'ç”µå½±æ„Ÿè°ƒè‰²', 'æ—¥ç³»å°æ¸…æ–°'
                ]
                # è¡¥å……å…œåº•å…³é”®è¯ï¼Œé¿å…é‡å¤
                for term in fallback_terms:
                    if term not in top_words:
                        top_words.append(term)
                    if len(top_words) >= 10:
                        break
            
            # åªä¿ç•™å‰10ä¸ªè¯
            top_words = top_words[:10]
            
            # 4. ç»ˆç«¯é”å®šï¼šæ‰“å°è¿™10ä¸ªè¯ï¼Œå¹¶æ‰§è¡Œinput
            print("\n" + "="*50)
            print("ğŸ¯ ä»50ä¸ªçˆ†æ¬¾å¸–æ ‡é¢˜ä¸­æçº¯å‡ºçš„10å¤§çƒ­é—¨é£æ ¼")
            print("="*50)
            for i, word in enumerate(top_words, 1):
                print(f"{i:2d}. {word}")
            print("="*50)
            
            # ç»ˆç«¯é”å®šï¼Œç­‰å¾…ç”¨æˆ·ç¡®è®¤
            input("è¿™æ˜¯ä» 50 ä¸ªçˆ†æ¬¾å¸–æ ‡é¢˜ä¸­æçº¯å‡ºçš„ 10 å¤§çƒ­é—¨é£æ ¼ï¼Œå¦‚éœ€ç»§ç»­è¯·æŒ‰å›è½¦...")
            
            print(f"âœ… æœ€ç»ˆç¡®å®šçš„10å¤§çƒ­é—¨é£æ ¼: {top_words}")
            return top_words
            
        except Exception as e:
            print(f"âŒ è·å–å…³è”æœç´¢è¯å¤±è´¥: {e}")
            # ä½¿ç”¨å…œåº•å…³é”®è¯
            fallback_terms = [
                'å¯Œå£«NCè°ƒè‰²', 'è«å…°è¿ªè‰²ç³»', 'æ³•å¼å¤å¤æ»¤é•œ', 'èƒ¶ç‰‡æ„Ÿè°ƒè‰²',
                'insé£æ»¤é•œ', 'å†·ç™½çš®è°ƒè‰²', 'å¥¶æ²¹è‚Œæ»¤é•œ', 'èµ›åšæœ‹å…‹è°ƒè‰²',
                'ç”µå½±æ„Ÿè°ƒè‰²', 'æ—¥ç³»å°æ¸…æ–°'
            ]
            print(f"ğŸ“‹ ä½¿ç”¨å…œåº•å…³é”®è¯: {fallback_terms}")
            return fallback_terms
    
    async def fetch_high_like_posts(self, page: Page, min_likes: int = 500, limit: int = 20) -> List[Dict[str, Any]]:
        """æŠ“å–é«˜èµå¸–å­ - å¤šé‡å¤‡é€‰é€»è¾‘æŠ“å–ç‚¹èµæ•°"""
        posts = []
        scroll_count = 0
        max_scrolls = 20
        processed_posts = set()  # å»é‡
        
        print(f"ğŸ“ å¼€å§‹æŠ“å–é«˜èµå¸–å­ï¼Œç›®æ ‡ï¼š{limit}æ¡ï¼Œç‚¹èµé˜ˆå€¼ï¼š{min_likes}")
        
        try:
            while len(posts) < limit and scroll_count < max_scrolls:
                print(f"ğŸ”„ æ»šåŠ¨ç¬¬{scroll_count+1}/{max_scrolls}æ¬¡")
                
                # æ£€æŸ¥é¡µé¢æ˜¯å¦å­˜æ´»
                try:
                    await page.evaluate("1+1")
                except Exception as e:
                    if "Target page closed" in str(e):
                        print("âš ï¸  é¡µé¢å·²å…³é—­ï¼Œåœæ­¢æŠ“å–")
                        return posts[:limit]
                    else:
                        raise
                
                # æ»šåŠ¨1500åƒç´ 
                await page.evaluate("window.scrollBy(0, 1500)")
                
                # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆï¼Œé™é€Ÿä¿ç¨³
                await page.wait_for_load_state("domcontentloaded")
                await page.wait_for_timeout(5000)  # é™é€Ÿä¿ç¨³
                
                # å®šä½å¸–å­å®¹å™¨ï¼š.note-item æˆ– section
                post_cards = await page.locator(".note-item, section").all()
                
                for card in post_cards:
                    if len(posts) >= limit:
                        break
                    
                    try:
                        # å»é‡ï¼šè·å–å¸–å­å”¯ä¸€æ ‡è¯†
                        card_id = await card.get_attribute("data-note-id") or await card.get_attribute("id")
                        if card_id and card_id in processed_posts:
                            continue
                        if card_id:
                            processed_posts.add(card_id)
                        
                        # æå–æ ‡é¢˜ï¼šspan.title æˆ– .footer .title
                        title = ""
                        try:
                            title = await card.locator("span.title").inner_text(timeout=5000)  # é™é€Ÿä¿ç¨³
                        except:
                            try:
                                title = await card.locator(".footer .title").inner_text(timeout=5000)  # é™é€Ÿä¿ç¨³
                            except:
                                title = "æ— æ ‡é¢˜"
                        
                        # æå–ç‚¹èµæ•°ï¼šå¤šé‡å¤‡é€‰é€»è¾‘
                        likes = 0
                        try:
                            # 1. ä¸»é€‰æ–¹æ¡ˆï¼šæŸ¥æ‰¾åŒ…å«æ•°å­—çš„ .count ç±»
                            # å®šä½æ‰€æœ‰.countå…ƒç´ ï¼Œç­›é€‰åŒ…å«æ•°å­—çš„
                            count_elements = await card.locator(".count").all()
                            likes_text = ""
                            
                            for element in count_elements:
                                text = await element.inner_text(timeout=5000)  # é™é€Ÿä¿ç¨³
                                if text and any(char.isdigit() for char in text):
                                    likes_text = text
                                    break
                            
                            if not likes_text:
                                # 2. å¤‡é€‰æ–¹æ¡ˆï¼šæŸ¥æ‰¾æ‰€æœ‰å¸¦æœ‰ç‚¹èµå›¾æ ‡ï¼ˆSVGï¼‰æ—è¾¹çš„æ–‡æœ¬æ ‡ç­¾
                                svg_elements = await card.locator("svg").all()
                                for svg in svg_elements:
                                    try:
                                        # æŸ¥æ‰¾SVGçš„ä¸‹ä¸€ä¸ªå…„å¼Ÿå…ƒç´ 
                                        next_sibling = await svg.locator("+ *").first.inner_text(timeout=5000)  # é™é€Ÿä¿ç¨³
                                        if next_sibling and any(char.isdigit() for char in next_sibling):
                                            likes_text = next_sibling
                                            break
                                    except:
                                        continue
                                
                                # å¤‡é€‰æ–¹æ¡ˆ2ï¼šæŸ¥æ‰¾åŒ…å«"èµ"æˆ–"heart"çš„å…ƒç´ 
                                try:
                                    like_elements = await card.locator("[class*='like'], [class*='heart']").all()
                                    for like_elem in like_elements:
                                        text = await like_elem.inner_text(timeout=5000)  # é™é€Ÿä¿ç¨³
                                        if text and any(char.isdigit() for char in text):
                                            likes_text = text
                                            break
                                except:
                                    pass
                            
                            # 3. æ¸…æ´—é€»è¾‘ï¼šå¤„ç†"ä¸‡"å­—
                            if likes_text:
                                # å»é™¤æ‰€æœ‰éæ•°å­—å’Œéç‚¹å­—ç¬¦
                                cleaned = ''.join(c for c in likes_text if c.isdigit() or c == '.')
                                if cleaned:
                                    if 'ä¸‡' in likes_text:
                                        likes = int(float(cleaned) * 10000)
                                    else:
                                        likes = int(float(cleaned))
                            
                        except Exception as e:
                            print(f"âš ï¸  ç‚¹èµæ•°æå–å¤±è´¥: {e}")
                            likes = 0  # å¦‚æœæŠ“å–ç»“æœä¸ºç©ºï¼Œé»˜è®¤ç»™0ï¼Œä¸è¦æŠ›å‡ºAttributeError
                        
                        # æå–å°é¢å›¾ï¼šimg.cover
                        cover_url = ""
                        try:
                            cover_url = await card.locator("img.cover").get_attribute("src", timeout=5000) or ""  # é™é€Ÿä¿ç¨³
                            if not cover_url:
                                # å°è¯•å…¶ä»–å°é¢å›¾å®šä½
                                cover_url = await card.locator("img").first.get_attribute("src", timeout=5000) or ""  # é™é€Ÿä¿ç¨³
                        except Exception as e:
                            print(f"âš ï¸  å°é¢å›¾æå–å¤±è´¥: {e}")
                            cover_url = ""  # å³ä½¿è¶…æ—¶ï¼Œä¹Ÿè¦ç»§ç»­æ‰§è¡Œ
                        
                        # æå–é“¾æ¥
                        link = ""
                        try:
                            link = await card.locator("a").first.get_attribute("href", timeout=5000) or ""  # é™é€Ÿä¿ç¨³
                        except Exception as e:
                            print(f"âš ï¸  é“¾æ¥æå–å¤±è´¥: {e}")
                            link = ""  # å³ä½¿è¶…æ—¶ï¼Œä¹Ÿè¦ç»§ç»­æ‰§è¡Œ
                        
                        # è®°å½•å¸–å­ï¼Œå³ä½¿éƒ¨åˆ†å­—æ®µç¼ºå¤±ä¹Ÿè¦è¿”å›
                        post_data = {
                            "title": title,
                            "likes": likes,
                            "cover_url": cover_url,
                            "link": f"https://www.xiaohongshu.com{link}" if link else ""
                        }
                        
                        # åªä¿ç•™ç‚¹èµæ•°å¤§äºmin_likesçš„å¸–å­åˆ°ç»“æœåˆ—è¡¨
                        # å³ä½¿å°é¢å›¾æˆ–é“¾æ¥ç¼ºå¤±ï¼Œåªè¦æœ‰ç‚¹èµå’Œæ ‡é¢˜å°±ä¿ç•™
                        if likes >= min_likes and title:
                            posts.append(post_data)
                        
                        print(f"ğŸ“‹ å¸–å­å¤„ç†å®Œæˆï¼šæ ‡é¢˜='{title[:20]}...', ç‚¹èµ={likes}")
                        
                    except Exception as e:
                        print(f"âš ï¸  å¤„ç†å¸–å­å¤±è´¥: {e}")
                        continue
                
                scroll_count += 1
            
            print(f"âœ… æŠ“å–åˆ° {len(posts)} æ¡é«˜èµå¸–å­")
            return posts[:limit]
            
        except Exception as e:
            print(f"âŒ æŠ“å–é«˜èµå¸–å­å¤±è´¥: {e}")
            return posts[:limit]
    
    def purify_aggregator(self, results: Dict[str, List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
        """æçº¯èšåˆæ•°æ®"""
        purified = []
        
        for term, posts in results.items():
            if posts:
                total_likes = sum(p["likes"] for p in posts)
                top_post = max(posts, key=lambda x: x["likes"])
                
                purified.append({
                    "term": term,
                    "total_likes": total_likes,
                    "top_post": top_post
                })
        
        return sorted(purified, key=lambda x: x["total_likes"], reverse=True)
    
    def generate_html_report(self, data: List[Dict[str, Any]]):
        """ç”ŸæˆHTMLæŠ¥å‘Š"""
        html = '''
        <!DOCTYPE html>
        <html lang="zh-CN">
        <head>
            <meta charset="UTF-8">
            <title>å°çº¢ä¹¦çœŸè¿¹å½’çº³æŠ¥å‘Š</title>
            <style>
                table { border-collapse: collapse; width: 100%; }
                th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
                th { background-color: #f2f2f2; }
                img { max-width: 100px; }
            </style>
        </head>
        <body>
            <h1>å°çº¢ä¹¦çœŸè¿¹å½’çº³æŠ¥å‘Š</h1>
            <table>
                <tr>
                    <th>æ’å</th>
                    <th>é£æ ¼</th>
                    <th>æ€»ç‚¹èµ</th>
                    <th>å°é¢å›¾</th>
                    <th>é“¾æ¥</th>
                </tr>
        '''
        
        for i, item in enumerate(data[:10], 1):
            html += '''
                <tr>
                    <td>%s</td>
                    <td>%s</td>
                    <td>%s</td>
                    <td><img src="%s"></td>
                    <td><a href="%s">æŸ¥çœ‹</a></td>
                </tr>
            ''' % (i, item['term'], item['total_likes'], item['top_post']['cover_url'], item['top_post']['link'])
        
        html += '''
            </table>
        </body>
        </html>
        '''
        
        with open("Artifact_Task_List.html", "w", encoding="utf-8") as f:
            f.write(html)


async def main():
    """ä¸»å‡½æ•°"""
    async with XiaoHongShuScraper() as scraper:
        print("ğŸ¯ çœŸè¿¹å½’çº³å¼•æ“å¯åŠ¨")
        
        # 1. åˆå§‹åŒ–ï¼šç›´æ¥åˆ›å»ºé¡µé¢
        initial_page = await scraper.context.new_page()
        await initial_page.goto("https://www.xiaohongshu.com/search_result?keyword=è°ƒè‰²")
        await initial_page.wait_for_load_state("domcontentloaded")
        await initial_page.wait_for_timeout(5000)  # é™é€Ÿä¿ç¨³
        
        # 2. æ£€æŸ¥ç™»å½•çŠ¶æ€ï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼Œæ— éœ€äººå·¥å¹²é¢„ï¼‰
        print("ğŸ” æ£€æŸ¥ç™»å½•çŠ¶æ€...")
        await initial_page.wait_for_load_state("domcontentloaded")
        await initial_page.wait_for_timeout(5000)  # é™é€Ÿä¿ç¨³
        
        # 3. è·å–å…³è”æœç´¢è¯
        print("ğŸ” å¼€å§‹è·å–å…³è”æœç´¢è¯...")
        related_terms = await scraper.get_related_search_terms(initial_page)
        
        # 4. å¦‚æœæœªè·å–åˆ°å…³è”æœç´¢è¯ï¼Œä½¿ç”¨å…œåº•å…³é”®è¯
        if not related_terms:
            print("âš ï¸  è·å–å…³è”æœç´¢è¯å¤±è´¥ï¼Œä½¿ç”¨å…œåº•å…³é”®è¯")
            related_terms = [
                'å¯Œå£«NCè°ƒè‰²', 'è«å…°è¿ªè‰²ç³»', 'æ³•å¼å¤å¤æ»¤é•œ', 'èƒ¶ç‰‡æ„Ÿè°ƒè‰²',
                'insé£æ»¤é•œ', 'å†·ç™½çš®è°ƒè‰²', 'å¥¶æ²¹è‚Œæ»¤é•œ', 'èµ›åšæœ‹å…‹è°ƒè‰²',
                'ç”µå½±æ„Ÿè°ƒè‰²', 'æ—¥ç³»å°æ¸…æ–°'  # é«˜äº§å‡ºå…œåº•è¯
            ]
        
        # 5. å¢åŠ åœé¡¿æ ¡éªŒï¼Œè®©ç”¨æˆ·ç¡®è®¤10å¤§æ ¸å¿ƒé£æ ¼
        print("\n" + "="*50)
        print("ğŸ¯ å¸‚åœºåˆ†æå‡ºçš„10å¤§æ ¸å¿ƒé£æ ¼")
        print("="*50)
        for i, term in enumerate(related_terms, 1):
            print(f"{i:2d}. {term}")
        print("="*50)
        input("ä»¥ä¸Šæ˜¯æ ¹æ®å¸‚åœºåˆ†æå‡ºçš„ 10 å¤§æ ¸å¿ƒé£æ ¼ï¼Œç¡®è®¤è¯·æŒ‰å›è½¦ï¼Œä¸æ»¡æ„è¯·å…³é—­ç¨‹åº...")
        
        print(f"âœ… å…³è”æœç´¢è¯: {related_terms}")
        
        # 5. å…³é—­åˆå§‹é¡µé¢
        await initial_page.close()
        
        # 6. æŠ“å–é«˜èµå¸–å­
        results = {}
        for term in related_terms:
            print(f"\nğŸ¯ å¤„ç†å…³é”®è¯: {term}")
            
            # æ£€æŸ¥æµè§ˆå™¨å’Œä¸Šä¸‹æ–‡æ˜¯å¦å­˜æ´»
            if not scraper.context or not scraper.browser:
                print("âš ï¸  æµè§ˆå™¨æˆ–ä¸Šä¸‹æ–‡å·²å…³é—­ï¼Œé‡æ–°åˆå§‹åŒ–")
                # é‡æ–°åˆå§‹åŒ–æµè§ˆå™¨å’Œä¸Šä¸‹æ–‡
                await scraper.__aexit__(None, None, None)
                await scraper.__aenter__()
            
            try:
                # ä¸ºæ¯ä¸ªå…³é”®è¯åˆ›å»ºæ–°é¡µé¢
                page = await scraper.context.new_page()
                try:
                    await page.goto(f"https://www.xiaohongshu.com/search_result?keyword={term}")
                    await page.wait_for_load_state("domcontentloaded")
                    await page.wait_for_timeout(5000)  # é™é€Ÿä¿ç¨³
                    
                    # æ£€æŸ¥ç™»å½•çŠ¶æ€ï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼Œæ— éœ€äººå·¥å¹²é¢„ï¼‰
                    print("ğŸ” æ£€æŸ¥ç™»å½•çŠ¶æ€...")
                    await page.wait_for_load_state("domcontentloaded")
                    await page.wait_for_timeout(5000)  # é™é€Ÿä¿ç¨³
                    
                    # æŠ“å–é«˜èµå¸–å­
                    posts = await scraper.fetch_high_like_posts(page)
                    results[term] = posts
                    print(f"âœ… æŠ“å–åˆ° {len(posts)} æ¡é«˜èµå¸–å­")
                    
                except Exception as e:
                    print(f"å¤„ç†å…³é”®è¯ {term} å¤±è´¥: {e}")
                finally:
                    await page.close()
            except Exception as e:
                print(f"åˆ›å»ºé¡µé¢å¤±è´¥: {e}")
                continue
        
        # 7. æçº¯èšåˆæ•°æ®
        print("\nğŸ“Š å¼€å§‹æçº¯èšåˆæ•°æ®...")
        purified_data = scraper.purify_aggregator(results)
        
        # 8. ç”ŸæˆHTMLæŠ¥å‘Š
        print("\nğŸ“ ç”ŸæˆHTMLæŠ¥å‘Š...")
        scraper.generate_html_report(purified_data)
        
        print("\nâœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")
        print("ğŸ“„ æŠ¥å‘Šè·¯å¾„: Artifact_Task_List.html")


if __name__ == "__main__":
    asyncio.run(main())
